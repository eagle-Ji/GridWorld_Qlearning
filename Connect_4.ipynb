{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Connect_4.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "N0tU3XF8jFUC",
        "djrFwlG4jRmU"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N0tU3XF8jFUC"
      },
      "source": [
        "### **Connect 4 Environment**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6uZLaIPBhSUp"
      },
      "source": [
        "import numpy as np\n",
        "import gym \n",
        "from gym import Env, spaces\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\"\"\"\n",
        "W = Win State \n",
        "L = Lose State\n",
        "A = Agent\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "# This grid settings are high dimension and difficult to render.\n",
        "\n",
        "Grid_Rows= 5\n",
        "Grid_Col = 5\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "# Use this setting to if you want to render the grid\n",
        "Grid_Rows= 10\n",
        "Grid_Col = 10\n",
        "WIN_STATE = (8, 6)\n",
        "LOSE_STATE = (9, 7)\n",
        "\"\"\"\n",
        "\n",
        "class connect_4_env:\n",
        "    \n",
        "    def __init__(self):\n",
        "        \n",
        "        self.all_actions = [0,1,2,3,4]\n",
        "        \n",
        "        self.action_space = spaces.Discrete(5)\n",
        "        self.observation_space = spaces.Box(low=np.array([0,0]), high = np.array([Grid_Rows, Grid_Col]), dtype=np.int16)\n",
        "        self.reward = 0\n",
        "        self.state = np.zeros([Grid_Rows, Grid_Col])\n",
        "        self.done = False\n",
        "        self.all_states = []\n",
        "        for i in range(0, Grid_Rows):\n",
        "             for j in range(0, Grid_Col):\n",
        "                 self.all_states.append((i,j))\n",
        "        \n",
        "    \n",
        "    def next_pos(self,state, action):\n",
        "       \n",
        "        \n",
        "        if action == 0:\n",
        "                next_position = (state[0] - 1, state[1])\n",
        "        elif action == 1:\n",
        "                next_position = (state[0] + 1, state[1])\n",
        "        elif action == 2:\n",
        "                next_position = (state[0], state[1] - 1)\n",
        "        elif action == 3:\n",
        "                next_position = (state[0], state[1] + 1)\n",
        "        else:\n",
        "            next_position = state\n",
        "            \n",
        "        # Keep the next state within specified range. \n",
        "        if (next_position[0] >= 0) and (next_position[0] <= (Grid_Rows -1)):\n",
        "            if (next_position[1] >= 0) and (next_position[1] <= (Grid_Col -1)):\n",
        "                \n",
        "                return next_position\n",
        "                    \n",
        "        return state\n",
        "    \n",
        "    def place_dot(self, player_n, position):\n",
        "        self.state[(position)] = player_n\n",
        "        \n",
        "    def possible_moves(self, state):\n",
        "        pos_act = []\n",
        "        for i in range(5):\n",
        "            position = self.next_pos(state, i)\n",
        "            \n",
        "            if self.state[(position)] == 0:\n",
        "                pos_act.append(i)\n",
        "        return pos_act\n",
        "    \n",
        "    def get_reward(self):\n",
        "        reward = 0\n",
        "        r1 = self.agent_reward()\n",
        "        r2 = self.self_reward()\n",
        "        if r1 == 4:\n",
        "            reward = r1\n",
        "        elif r2 == -4:\n",
        "            reward = r2\n",
        "        else:\n",
        "            reward = 0\n",
        "        return reward\n",
        "    def agent_reward(self):\n",
        "            all_point = []\n",
        "            reward = 0\n",
        "            \n",
        "            for i in range(0, Grid_Rows):\n",
        "                if reward == 4:\n",
        "                    #self.done = True\n",
        "                    break\n",
        "                for j in range(0, Grid_Col):\n",
        "                    if self.state[(i,j)] == 1:\n",
        "                        point = (i,j)\n",
        "                        all_point.append((i,j))\n",
        "                        for p in range(4):\n",
        "                            if (point[1] + p) < Grid_Col: \n",
        "                                if self.state[(point[0], (point[1] + p))] == 1:\n",
        "                                    reward += 1\n",
        "                                else:\n",
        "                                    reward = 0\n",
        "                                    break\n",
        "                            else:\n",
        "                                reward = 0\n",
        "                                break\n",
        "                        break\n",
        "            all_point = []      \n",
        "            for j in range(0, Grid_Col):\n",
        "                if reward == 4:\n",
        "                    #self.done = True\n",
        "                    break\n",
        "                for i in range(0, Grid_Rows):\n",
        "                    if self.state[(i,j)] == 1:\n",
        "                        point = (i,j)\n",
        "                        all_point.append((i,j))\n",
        "                        for p in range(4):\n",
        "                            if (point[0] + p) < Grid_Rows:\n",
        "                                if self.state[((point[0]+p), point[1])] == 1:\n",
        "                                    reward += 1\n",
        "                                else:\n",
        "                                    reward = 0\n",
        "                                    break\n",
        "                            else:\n",
        "                                reward = 0\n",
        "                                break\n",
        "                        break\n",
        "            \n",
        "            if reward == 4:\n",
        "                    self.done = True\n",
        "            \n",
        "            return reward\n",
        "\n",
        "    def self_reward(self):\n",
        "            all_point = []\n",
        "            reward = 0\n",
        "            \n",
        "            for i in range(0, Grid_Rows):\n",
        "                if reward == 4:\n",
        "                    #self.done = True\n",
        "                    break\n",
        "                for j in range(0, Grid_Col):\n",
        "                    if self.state[(i,j)] == 2:\n",
        "                        point = (i,j)\n",
        "                        all_point.append((i,j))\n",
        "                        for p in range(4):\n",
        "                            if (point[1] + p) < Grid_Col: \n",
        "                                if self.state[(point[0], (point[1] + p))] == 2:\n",
        "                                    reward -= 1\n",
        "                                else:\n",
        "                                    reward = 0\n",
        "                                    break\n",
        "                            else:\n",
        "                                reward = 0\n",
        "                                break\n",
        "                        break\n",
        "            all_point = []      \n",
        "            for j in range(0, Grid_Col):\n",
        "                if reward == 4:\n",
        "                    #self.done = True\n",
        "                    break\n",
        "                for i in range(0, Grid_Rows):\n",
        "                    if self.state[(i,j)] == 2:\n",
        "                        point = (i,j)\n",
        "                        all_point.append((i,j))\n",
        "                        for p in range(4):\n",
        "                            if (point[0] + p) < Grid_Rows:\n",
        "                                if self.state[((point[0]+p), point[1])] == 2:\n",
        "                                    reward -= 1\n",
        "                                else:\n",
        "                                    reward = 0\n",
        "                                    break\n",
        "                            else:\n",
        "                                reward = 0\n",
        "                                break\n",
        "                        break\n",
        "            \n",
        "            if reward == -4:\n",
        "                self.done = True\n",
        "            \n",
        "            return reward\n",
        "\n",
        "    def empty_states(self):\n",
        "        e_state = []\n",
        "        for i in range(0, Grid_Rows):\n",
        "            for j in range(0, Grid_Col):\n",
        "                if self.state[i,j] == 0:\n",
        "                    e_state.append((i,j))\n",
        "        return e_state\n",
        "    \n",
        "    def self_play(self, state):\n",
        "        p_act = []\n",
        "        position = []\n",
        "        e_state = self.empty_states()\n",
        "        if (e_state != []):\n",
        "            p_act = self.possible_moves(state)\n",
        "            \n",
        "            if (p_act != []):\n",
        "                act = random.choice(p_act)\n",
        "                position = self.next_pos(state, act)\n",
        "            else:\n",
        "                if (state in e_state):\n",
        "                    self.place_dot(2,state)\n",
        "                \n",
        "                \n",
        "            if (position in e_state):\n",
        "                self.place_dot(2,position)\n",
        "            else:\n",
        "                position = random.choice(e_state)\n",
        "                self.place_dot(2,position)\n",
        "        else:\n",
        "            self.done = True \n",
        "            #print(\"ALL Full\")\n",
        "        #print(p_act, act, position)\n",
        "        return position\n",
        "    \n",
        "    def agent_play(self, state):\n",
        "        p_act = []\n",
        "        position = []\n",
        "        e_state = self.empty_states()\n",
        "        if (e_state != []):\n",
        "            p_act = self.possible_moves(state)\n",
        "            \n",
        "            if (p_act != []):\n",
        "                act = random.choice(p_act)\n",
        "                position = self.next_pos(state, act)\n",
        "            else:\n",
        "                if (state in e_state):\n",
        "                    self.place_dot(1,state)\n",
        "                \n",
        "                \n",
        "            if (position in e_state):\n",
        "                self.place_dot(1,position)\n",
        "            else:\n",
        "                position = random.choice(e_state)\n",
        "                self.place_dot(1,position)\n",
        "        else:\n",
        "            self.done = True \n",
        "            #print(\"ALL Full\")\n",
        "        #print(p_act, act, position)\n",
        "        return position\n",
        "        \n",
        "        \n",
        "    def take_action(self, state, act):\n",
        "        position = self.next_pos(state, act)\n",
        "        self.place_dot(1,position)\n",
        "        \n",
        "        self.self_play(position)\n",
        "        reward = self.get_reward()\n",
        "        return position, reward\n",
        "        \n",
        "        \n",
        "    def get_state_actions(self, state):\n",
        "        #e_state = self.empty_states()\n",
        "        p_act = self.possible_moves(state)\n",
        "        return p_act\n",
        "        \n",
        "        \n",
        "        \n",
        "    def get_next_state(self):\n",
        "        e_state = self.empty_states()\n",
        "        if e_state != []:\n",
        "            state = random.choice(e_state)\n",
        "        else:\n",
        "            state = []\n",
        "            self.done = True\n",
        "        return state\n",
        "        \n",
        "    \n",
        "        \n",
        "    \n",
        "    def render(self, next_state):\n",
        "        print(\"---------------------------------\")\n",
        "        print(\"Current State is \", np.asarray(next_state))\n",
        "        print(\"The reward is   \",self.reward)\n",
        "        print(\"---------------------------------\")\n",
        "        \n",
        "    def reset(self):\n",
        "        self.reward = 0\n",
        "        self.state = np.zeros([Grid_Rows, Grid_Col])\n",
        "        self.done = False\n",
        "        return (np.random.randint(0, Grid_Rows), np.random.randint(0, Grid_Col))\n",
        "        \n",
        "        \n",
        "    def render_grid(self, state):\n",
        "        print(\"Agent = X :: Computer = #\")\n",
        "        for i in range(0, Grid_Rows):\n",
        "            print(\"---------------------\")\n",
        "            out = '| '\n",
        "            for j in range(0, Grid_Col):\n",
        "                if self.state[i, j] == 1:\n",
        "                    token = 'X'\n",
        "                if self.state[i, j] == 2:\n",
        "                    token = '#'\n",
        "                if self.state[i, j] == 0:\n",
        "                    token = ' '\n",
        "                \n",
        "                out += token + ' | '\n",
        "            print(out)\n",
        "        print(\"---------------------\")\n",
        "        \n",
        "        \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_4CLE7xohqY8"
      },
      "source": [
        "# Test Section\n",
        "# In this random actions are taken so moved may be missed by the agent\n",
        "env = connect_4_env()\n",
        "state = env.reset()\n",
        "rew = []\n",
        "win_count = 0\n",
        "tie_count = 0\n",
        "lose_count = 0\n",
        "step_count = []\n",
        "win_game = []\n",
        "tie_game = []\n",
        "lose_game = []\n",
        "\n",
        "for e in range(1):\n",
        "    state = env.reset()\n",
        "    win_game.append(win_count)\n",
        "    tie_game.append(tie_count)\n",
        "    lose_game.append(lose_count)\n",
        "    for s in range(5):\n",
        "        #action = optimal_policy[state]\n",
        "        action = env.action_space.sample()\n",
        "        next_state, reward = env.take_action(state, action)\n",
        "        env.render_grid(next_state)   \n",
        "        rew.append(reward)\n",
        "        state = env.get_next_state()\n",
        "        \n",
        "        if env.done:\n",
        "            #print(f\"Solved in Steps {s}\")\n",
        "            step_count.append(s)\n",
        "            if reward == 4:\n",
        "                #print(\"WIN\")\n",
        "                win_count += 1\n",
        "            elif reward == -4:\n",
        "                #print(\"Loss\")\n",
        "                lose_count += 1\n",
        "            else:\n",
        "                #print(\"TIE\")\n",
        "                tie_count += 1\n",
        "            break\n",
        "    \n",
        "\n",
        "\n",
        "print(\"Game Results Random Policy\")\n",
        "print(f\"Total Games played 100: [Win Games {win_count}]: [Lose Games {lose_count}]: [Tie Games {tie_count}]\")\n",
        "\n",
        "print(f\"Random Policy Accuracy  [{(win_count/100)*100}%]\")  \n",
        "#Visualization\n",
        "\n",
        "plt.figure(figsize=(10,5)) \n",
        "plt.plot(win_game, \"g--\", markersize=20, linewidth=5)\n",
        "plt.plot(lose_game, \"r--\", markersize=20, linewidth=5)\n",
        "plt.plot(tie_game, \"b--\", markersize=20, linewidth=5)\n",
        "plt.legend([\"Win\", \"Loss\", \"Tie\"], fontsize=20)\n",
        "plt.title(\"Games Results [Random Policy] : Total Games = 100\", fontsize=20)\n",
        "plt.xlabel(\"Game Count\", fontsize=20)\n",
        "plt.ylabel(\"Win Count\", fontsize=20)\n",
        "plt.xlim(0,100)\n",
        "plt.grid()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "#avg_step = sum(step_count)/100\n",
        "#print(f\"Average Steps to win 100 games = {round(avg_step)}\")\n",
        "\n",
        "env.render_grid(state)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "djrFwlG4jRmU"
      },
      "source": [
        "### **Value Iteration**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VhnWtisCjKNS"
      },
      "source": [
        "\n",
        "Q_table = {}\n",
        "for i in range(Grid_Rows):\n",
        "    for j in range(Grid_Col):\n",
        "        Q_table[(i, j)] = [0,0,0,0,0]\n",
        "\n",
        "env = connect_4_env()\n",
        "\n",
        "def get_value_table(env):\n",
        "\n",
        "    #set the number of iterations\n",
        "    num_iterations = 1000\n",
        "    \n",
        "    #set the threshold number for checking the convergence of the value function\n",
        "    threshold = 1e-40\n",
        "    con_value = 0\n",
        "    c_act = 0\n",
        "    \n",
        "    #we also set the discount factor\n",
        "    gamma = 1.0\n",
        "    \n",
        "    #now, we will initialize the value table, with the value of all states to zero\n",
        "    value_table = np.zeros((Grid_Rows, Grid_Col))\n",
        "    \n",
        "    #for every iteration\n",
        "    for i in range(num_iterations):\n",
        "        \n",
        "        #update the value table, that is, we learned that on every iteration, we use the updated value\n",
        "        #table (state values) from the previous iteration\n",
        "        \n",
        "        updated_value_table = np.copy(value_table)\n",
        "        all_states = env.empty_states()\n",
        "        if all_states != []:\n",
        "            for s in all_states:\n",
        "                p_act = env.possible_moves(s)\n",
        "                for a in p_act:\n",
        "                    s_, reward = env.take_action(s, a)                \n",
        "                    Q_table[s][a] = reward + gamma * updated_value_table[s_]\n",
        "                value_table[s] = max(Q_table[s])\n",
        "            if (np.sum(np.fabs(updated_value_table - value_table)) <= threshold):\n",
        "                c_act = a\n",
        "                con_value = i\n",
        "                \n",
        "                break    \n",
        "        else:\n",
        "            env.reset()\n",
        "                            \n",
        "        \n",
        "    \n",
        "        \n",
        "    \n",
        "    return value_table, con_value, c_act\n",
        "\n",
        "def get_policy(value_table):\n",
        "    \n",
        "    #set the discount factor\n",
        "    gamma = 1.0\n",
        "     \n",
        "\n",
        "    policy = np.zeros((Grid_Rows, Grid_Col))\n",
        "    \n",
        "    \n",
        "    #for each state\n",
        "    all_states = env.empty_states()\n",
        "    if all_states != []:\n",
        "        for s in all_states:\n",
        "            p_act = env.possible_moves(s)\n",
        "            for a in p_act:\n",
        "                s_, reward = env.take_action(s, a)                \n",
        "                Q_table[s][a] = reward + gamma * value_table[s_] \n",
        "            policy[s] = np.argmax(Q_table[s])    \n",
        "    else:\n",
        "        env.reset()\n",
        "    \n",
        "    \n",
        "    return policy\n",
        "\n",
        "\n",
        "\n",
        "convergance_value = []\n",
        "calculate_conv = []\n",
        "con_answer = []\n",
        "for rep in range(1000):\n",
        "    value_table, con_value, c_act = get_value_table(env) # Based on value iteration\n",
        "    \n",
        "    optimal_policy = get_policy(value_table)\n",
        "    calculate_conv.append(con_value)\n",
        "    \n",
        "    if rep % 50 == 0:\n",
        "        val = (sum(calculate_conv)/50)\n",
        "        convergance_value.append(val)\n",
        "        calculate_conv = []\n",
        "        con_answer.append(c_act)\n",
        "        #print(val)\n",
        "        \n",
        "        \n",
        "\n",
        "print(\"Computed Value Table based on Value Iteration\")\n",
        "print(value_table)\n",
        " # Based on value table\n",
        "print(\"Optimal Policy Table based on Value Iteration\")\n",
        "print(optimal_policy)\n",
        "\n",
        "print(\"Average value for convergence \", round((sum(convergance_value)/len(convergance_value))))\n",
        "\n",
        "plt.figure(figsize=(10,5)) \n",
        "plt.plot(convergance_value, \"g--\", markersize=20, linewidth=5)\n",
        "plt.legend([\"Convergence Value\"], fontsize=20)\n",
        "plt.title(\"Iteration taken to converge [Value Iteration]\", fontsize=20)\n",
        "plt.xlabel(\"Repeatations\", fontsize=20)\n",
        "plt.ylabel(\"Convergence Point\", fontsize=20)\n",
        "plt.xlim(0,20)\n",
        "plt.grid()\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(10,5)) \n",
        "plt.plot(con_answer, \"g--\", markersize=20, linewidth=5)\n",
        "plt.legend([\"Convergence Answer\"], fontsize=20)\n",
        "plt.title(\"Action Value at convergence [Value Iteration]\", fontsize=20)\n",
        "plt.xlabel(\"Repeatations\", fontsize=20)\n",
        "plt.ylabel(\"Action Value\", fontsize=20)\n",
        "plt.xlim(0,20)\n",
        "plt.grid()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "state = env.reset()\n",
        "rew = []\n",
        "win_count = 0\n",
        "tie_count = 0\n",
        "lose_count = 0\n",
        "step_count = []\n",
        "win_game = []\n",
        "tie_game = []\n",
        "lose_game = []\n",
        "\n",
        "for e in range(100):\n",
        "    state = env.reset()\n",
        "    win_game.append(win_count)\n",
        "    tie_game.append(tie_count)\n",
        "    lose_game.append(lose_count)\n",
        "    for s in range(50):\n",
        "        #action = optimal_policy[state]\n",
        "        action = env.action_space.sample()\n",
        "        next_state, reward = env.take_action(state, action)\n",
        "        \n",
        "        rew.append(reward)\n",
        "        state = env.get_next_state()\n",
        "        if env.done:\n",
        "            #print(f\"Solved in Steps {s}\")\n",
        "            step_count.append(s)\n",
        "            if reward == 4:\n",
        "                #print(\"WIN\")\n",
        "                win_count += 1\n",
        "            elif reward == -4:\n",
        "                #print(\"Loss\")\n",
        "                lose_count += 1\n",
        "            else:\n",
        "                #print(\"TIE\")\n",
        "                tie_count += 1\n",
        "            break\n",
        "    \n",
        "\n",
        "    \n",
        "#Visualization\n",
        "\n",
        "plt.figure(figsize=(10,5)) \n",
        "plt.plot(win_game, \"g--\", markersize=20, linewidth=5)\n",
        "plt.plot(lose_game, \"r--\", markersize=20, linewidth=5)\n",
        "plt.plot(tie_game, \"b--\", markersize=20, linewidth=5)\n",
        "plt.legend([\"Win\", \"Loss\", \"Tie\"], fontsize=20)\n",
        "plt.title(\"Games Results [Random Policy] : Total Games = 100\", fontsize=20)\n",
        "plt.xlabel(\"Game Count\", fontsize=20)\n",
        "plt.ylabel(\"Win Count\", fontsize=20)\n",
        "plt.xlim(0,100)\n",
        "plt.grid()\n",
        "plt.show()\n",
        "\n",
        "print(\"Game Results\")\n",
        "print(f\"Total Games played 100: [Win Games {win_count}]: [Lose Games {lose_count}]: [Tie Games {tie_count}]\")\n",
        "\n",
        "print(f\"Accuracy of Play{(win_count/100)*100}%\")\n",
        "\n",
        "\n",
        "#avg_step = sum(step_count)/100\n",
        "#print(f\"Average Steps to win 100 games = {round(avg_step)}\")\n",
        "\n",
        "env.render_grid(state)\n",
        "\n",
        "\n",
        "\n",
        "state = env.reset()\n",
        "rew = []\n",
        "win_count = 0\n",
        "tie_count = 0\n",
        "lose_count = 0\n",
        "step_count = []\n",
        "win_game = []\n",
        "tie_game = []\n",
        "lose_game = []\n",
        "\n",
        "for e in range(100):\n",
        "    state = env.reset()\n",
        "    win_game.append(win_count)\n",
        "    tie_game.append(tie_count)\n",
        "    lose_game.append(lose_count)\n",
        "    for s in range(50):\n",
        "        action = optimal_policy[state]\n",
        "        \n",
        "        next_state, reward = env.take_action(state, action)\n",
        "        \n",
        "        rew.append(reward)\n",
        "        state = env.get_next_state()\n",
        "        if env.done:\n",
        "            #print(f\"Solved in Steps {s}\")\n",
        "            step_count.append(s)\n",
        "            if reward == 4:\n",
        "                #print(\"WIN\")\n",
        "                win_count += 1\n",
        "            elif reward == -4:\n",
        "                #print(\"Loss\")\n",
        "                lose_count += 1\n",
        "            else:\n",
        "                #print(\"TIE\")\n",
        "                tie_count += 1\n",
        "            break\n",
        "    \n",
        "\n",
        "    \n",
        "#Visualization\n",
        "\n",
        "plt.figure(figsize=(10,5)) \n",
        "plt.plot(win_game, \"g--\", markersize=20, linewidth=5)\n",
        "plt.plot(lose_game, \"r--\", markersize=20, linewidth=5)\n",
        "plt.plot(tie_game, \"b--\", markersize=20, linewidth=5)\n",
        "plt.legend([\"Win\", \"Loss\", \"Tie\"], fontsize=20)\n",
        "plt.title(\"Games Results Optimal Policy [Value Iteration] : Total Games = 100\", fontsize=20)\n",
        "plt.xlabel(\"Game Count\", fontsize=20)\n",
        "plt.ylabel(\"Win Count\", fontsize=20)\n",
        "plt.xlim(0,100)\n",
        "plt.grid()\n",
        "plt.show()\n",
        "\n",
        "print(\"Game Results\")\n",
        "print(f\"Total Games played 100: [Win Games {win_count}]: [Lose Games {lose_count}]: [Tie Games {tie_count}]\")\n",
        "\n",
        "print(f\"Accuracy of Play{(win_count/100)*100}%\")\n",
        "\n",
        "env.render_grid(state)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UMckgQJnjtkJ"
      },
      "source": [
        "### **Policy Iteration**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ARqfHY9njxNj"
      },
      "source": [
        "\n",
        "Q_table = {}\n",
        "for i in range(Grid_Rows):\n",
        "    for j in range(Grid_Col):\n",
        "        Q_table[(i, j)] = [0,0,0,0,0]\n",
        "\n",
        "env = connect_4_env()\n",
        "\n",
        "def get_value_table(policy):\n",
        "\n",
        "    #set the number of iterations\n",
        "    num_iterations = 1000\n",
        "    \n",
        "    #set the threshold number for checking the convergence of the value function\n",
        "    threshold = 1e-20\n",
        "    \n",
        "    #we also set the discount factor\n",
        "    gamma = 1.0\n",
        "    \n",
        "    #now, we will initialize the value table, with the value of all states to zero\n",
        "    value_table = np.zeros((Grid_Rows, Grid_Col))\n",
        "    \n",
        "    #for every iteration\n",
        "    for i in range(num_iterations):\n",
        "        \n",
        "        #update the value table, that is, we learned that on every iteration, we use the updated value\n",
        "        #table (state values) from the previous iteration\n",
        "        updated_value_table = np.copy(value_table)\n",
        "        all_states = env.empty_states()\n",
        "        if all_states != []:\n",
        "            for s in all_states:\n",
        "                    a = policy[s]\n",
        "                    s_, reward = env.take_action(s, a)                \n",
        "                    value_table[s] = reward + gamma * updated_value_table[s_]\n",
        "                \n",
        "                \n",
        "        else:\n",
        "            env.reset()\n",
        "                            \n",
        "        \n",
        "    \n",
        "        if (np.sum(np.fabs(updated_value_table - value_table)) <= threshold):\n",
        "             break\n",
        "    \n",
        "    return value_table\n",
        "\n",
        "def get_value_table(policy):\n",
        "    \n",
        "    #set the number of iterations\n",
        "    num_iterations = 1000\n",
        "    \n",
        "    #set the threshold number for checking the convergence of the value function\n",
        "    threshold = 1e-40\n",
        "    \n",
        "    #we also set the discount factor\n",
        "    gamma = 1.0\n",
        "    \n",
        "    #now, we will initialize the value table, with the value of all states to zero\n",
        "    value_table = np.zeros((Grid_Rows, Grid_Col))\n",
        "    \n",
        "    #for every iteration\n",
        "    for i in range(num_iterations):\n",
        "        #table (state values) from the previous iteration\n",
        "        updated_value_table = np.copy(value_table)\n",
        "        all_states = env.empty_states()\n",
        "        if all_states != []:\n",
        "            for s in all_states:\n",
        "                    a = policy[s]\n",
        "                    s_, reward = env.take_action(s, a)                \n",
        "                    value_table[s] = reward + gamma * updated_value_table[s_]\n",
        "                \n",
        "                \n",
        "        else:\n",
        "            env.reset()\n",
        "                            \n",
        "        \n",
        "    \n",
        "        if (np.sum(np.fabs(updated_value_table - value_table)) <= threshold):\n",
        "             \n",
        "            break\n",
        "    \n",
        "    return value_table\n",
        "\n",
        "def get_policy(value_table):\n",
        "    \n",
        "    #set the discount factor\n",
        "    gamma = 1.0\n",
        "     \n",
        "\n",
        "    policy = np.zeros((Grid_Rows, Grid_Col))\n",
        "    \n",
        " \n",
        "    \n",
        "    #for each state\n",
        "    all_states = env.empty_states()\n",
        "    if all_states != []:\n",
        "        for s in all_states:\n",
        "            p_act = env.possible_moves(s)\n",
        "            for a in p_act:\n",
        "                s_, reward = env.take_action(s, a)                \n",
        "                Q_table[s][a] = reward + gamma * value_table[s_] \n",
        "            policy[s] = np.argmax(Q_table[s])    \n",
        "    else:\n",
        "        env.reset()\n",
        "    \n",
        "    \n",
        "    return policy, a\n",
        "\n",
        "\n",
        "def policy_iteration(env):\n",
        "    con_value = 0\n",
        "    c_act = 0\n",
        "    num_iterations = 1000\n",
        "    policy_  = np.zeros((Grid_Rows, Grid_Col))\n",
        "    for i in range(num_iterations):\n",
        "        value_function = get_value_table(policy_)\n",
        "        new_policy, a = get_policy(value_function)\n",
        "        if (np.all(policy_ == new_policy)):\n",
        "            c_act = a\n",
        "            con_value = i\n",
        "            break\n",
        "        policy_ = new_policy\n",
        "    return policy_, con_value, a\n",
        "\n",
        "\n",
        "convergance_value = []\n",
        "calculate_conv = []\n",
        "con_answer = []\n",
        "for rep in range(1000):\n",
        "    optimal_policy, con_value, a = policy_iteration(env)\n",
        "    calculate_conv.append(con_value)\n",
        "    \n",
        "    if rep % 50 == 0:\n",
        "        val = (sum(calculate_conv)/50)\n",
        "        convergance_value.append(val)\n",
        "        calculate_conv = []\n",
        "        con_answer.append(a)\n",
        "\n",
        "\n",
        "print(\"Optimal Policy Table based on Policy Iteration\")\n",
        "print(optimal_policy)\n",
        "\n",
        "print(\"Average value for convergence \", round((sum(convergance_value)/len(convergance_value))))\n",
        "\n",
        "plt.figure(figsize=(10,5)) \n",
        "plt.plot(convergance_value, \"g--\", markersize=20, linewidth=5)\n",
        "plt.legend([\"Convergence Value\"], fontsize=20)\n",
        "plt.title(\"Iteration taken to converge [Policy Iteration]\", fontsize=20)\n",
        "plt.xlabel(\"Repeatations\", fontsize=20)\n",
        "plt.ylabel(\"Convergence Point\", fontsize=20)\n",
        "plt.xlim(0,20)\n",
        "plt.grid()\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(10,5)) \n",
        "plt.plot(con_answer, \"g--\", markersize=20, linewidth=5)\n",
        "plt.legend([\"Convergence Answer\"], fontsize=20)\n",
        "plt.title(\"Action Value at convergence [Policy Iteration]\", fontsize=20)\n",
        "plt.xlabel(\"Repeatations\", fontsize=20)\n",
        "plt.ylabel(\"Action Value\", fontsize=20)\n",
        "plt.xlim(0,20)\n",
        "plt.grid()\n",
        "plt.show()\n",
        "\n",
        "#np.save(\"Convergance_Action_Policy_Itetration.npy\",con_answer , allow_pickle=True)\n",
        "#np.save(\"Convergance_Value_Policy_Itetration.npy\", convergance_value, allow_pickle=True)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "state = env.reset()\n",
        "rew = []\n",
        "win_count = 0\n",
        "tie_count = 0\n",
        "lose_count = 0\n",
        "step_count = []\n",
        "win_game = []\n",
        "tie_game = []\n",
        "lose_game = []\n",
        "\n",
        "for e in range(100):\n",
        "    state = env.reset()\n",
        "    win_game.append(win_count)\n",
        "    tie_game.append(tie_count)\n",
        "    lose_game.append(lose_count)\n",
        "    for s in range(50):\n",
        "        #action = optimal_policy[state]\n",
        "        action = env.action_space.sample()\n",
        "        next_state, reward = env.take_action(state, action)\n",
        "        \n",
        "        rew.append(reward)\n",
        "        state = env.get_next_state()\n",
        "        if env.done:\n",
        "            #print(f\"Solved in Steps {s}\")\n",
        "            step_count.append(s)\n",
        "            if reward == 4:\n",
        "                #print(\"WIN\")\n",
        "                win_count += 1\n",
        "            elif reward == -4:\n",
        "                #print(\"Loss\")\n",
        "                lose_count += 1\n",
        "            else:\n",
        "                #print(\"TIE\")\n",
        "                tie_count += 1\n",
        "            break\n",
        "    \n",
        "\n",
        "\n",
        "print(\"Game Results Random Policy\")\n",
        "print(f\"Total Games played 100: [Win Games {win_count}]: [Lose Games {lose_count}]: [Tie Games {tie_count}]\")\n",
        "\n",
        "print(f\"Random Policy Accuracy  [{(win_count/100)*100}%]\")  \n",
        "#Visualization\n",
        "\n",
        "plt.figure(figsize=(10,5)) \n",
        "plt.plot(win_game, \"g--\", markersize=20, linewidth=5)\n",
        "plt.plot(lose_game, \"r--\", markersize=20, linewidth=5)\n",
        "plt.plot(tie_game, \"b--\", markersize=20, linewidth=5)\n",
        "plt.legend([\"Win\", \"Loss\", \"Tie\"], fontsize=20)\n",
        "plt.title(\"Games Results [Random Policy] : Total Games = 100\", fontsize=20)\n",
        "plt.xlabel(\"Game Count\", fontsize=20)\n",
        "plt.ylabel(\"Win Count\", fontsize=20)\n",
        "plt.xlim(0,100)\n",
        "plt.grid()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "#avg_step = sum(step_count)/100\n",
        "#print(f\"Average Steps to win 100 games = {round(avg_step)}\")\n",
        "\n",
        "env.render_grid(state)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "state = env.reset()\n",
        "rew = []\n",
        "win_count = 0\n",
        "tie_count = 0\n",
        "lose_count = 0\n",
        "step_count = []\n",
        "win_game = []\n",
        "tie_game = []\n",
        "lose_game = []\n",
        "\n",
        "for e in range(100):\n",
        "    state = env.reset()\n",
        "    win_game.append(win_count)\n",
        "    tie_game.append(tie_count)\n",
        "    lose_game.append(lose_count)\n",
        "    for s in range(50):\n",
        "        action = optimal_policy[state]\n",
        "        \n",
        "        next_state, reward = env.take_action(state, action)\n",
        "        \n",
        "        rew.append(reward)\n",
        "        state = env.get_next_state()\n",
        "        if env.done:\n",
        "            #print(f\"Solved in Steps {s}\")\n",
        "            step_count.append(s)\n",
        "            if reward == 4:\n",
        "                #print(\"WIN\")\n",
        "                win_count += 1\n",
        "            elif reward == -4:\n",
        "                #print(\"Loss\")\n",
        "                lose_count += 1\n",
        "            else:\n",
        "                #print(\"TIE\")\n",
        "                tie_count += 1\n",
        "            break\n",
        "    \n",
        "\n",
        "\n",
        "print(\"Game Results Optimal Policy\")\n",
        "print(f\"Total Games played 100: [Win Games {win_count}]: [Lose Games {lose_count}]: [Tie Games {tie_count}]\")\n",
        "\n",
        "print(f\"Policy Iteration Accuracy  [{(win_count/100)*100}%]\") \n",
        "   \n",
        "#Visualization\n",
        "\n",
        "plt.figure(figsize=(10,5)) \n",
        "plt.plot(win_game, \"g--\", markersize=20, linewidth=5)\n",
        "plt.plot(lose_game, \"r--\", markersize=20, linewidth=5)\n",
        "plt.plot(tie_game, \"b--\", markersize=20, linewidth=5)\n",
        "plt.legend([\"Win\", \"Loss\", \"Tie\"], fontsize=20)\n",
        "plt.title(\"Games Results Optimal Policy [Policy Iteration] : Total Games = 100\", fontsize=20)\n",
        "plt.xlabel(\"Game Count\", fontsize=20)\n",
        "plt.ylabel(\"Win Count\", fontsize=20)\n",
        "plt.xlim(0,100)\n",
        "plt.grid()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n",
        "env.render_grid(state)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}