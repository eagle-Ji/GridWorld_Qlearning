{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Grid_Word.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "zqciToT4f8pm"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zqciToT4f8pm"
      },
      "source": [
        "## **Grid Word**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fJs8hnjIdk6i"
      },
      "source": [
        "import numpy as np\n",
        "import gym \n",
        "from gym import Env, spaces\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "W = Win State \n",
        "L = Lose State\n",
        "A = Agent\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "# Highest grid settings\n",
        "Grid_Rows= 50\n",
        "Grid_Col = 50\n",
        "WIN_STATE = (20, 20)\n",
        "LOSE_STATE = (19, 19)\n",
        "\"\"\"\n",
        "# This grid settings are high dimension and difficult to render.\n",
        "# Required Grid Settings\n",
        "Grid_Rows= 16\n",
        "Grid_Col = 16\n",
        "WIN_STATE = (12, 13)\n",
        "LOSE_STATE = (13, 14)\n",
        "\n",
        "\n",
        "# Lowest grid settings\n",
        "# Use this setting to if you want to render the grid\n",
        "Grid_Rows= 10\n",
        "Grid_Col = 10\n",
        "WIN_STATE = (8, 6)\n",
        "LOSE_STATE = (9, 7)\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "class grid_word:\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.print_grid = np.zeros([Grid_Rows, Grid_Col])\n",
        "        self.print_grid[WIN_STATE] = 9\n",
        "        self.print_grid[LOSE_STATE] = 2\n",
        "        self.state = (np.random.randint(0, Grid_Rows), np.random.randint(0, Grid_Col))\n",
        "        self.done = False\n",
        "        self.all_actions = [0,1,2,3]\n",
        "        \n",
        "        self.action_space = spaces.Discrete(4)\n",
        "        self.observation_space = spaces.Box(low=np.array([0,0]), high = np.array([Grid_Rows, Grid_Col]), dtype=np.int16)\n",
        "        self.reward = 0\n",
        "        self.P = {}\n",
        "        for i in range(Grid_Rows):\n",
        "            for j in range(Grid_Col):\n",
        "                self.P[(i, j)] = [0.25,0.25,0.25,0.25]\n",
        "    \n",
        "    def move(self,state, action):\n",
        "       \n",
        "        if action == 0:\n",
        "                next_position = (state[0] - 1, state[1])\n",
        "        elif action == 1:\n",
        "                next_position = (state[0] + 1, state[1])\n",
        "        elif action == 2:\n",
        "                next_position = (state[0], state[1] - 1)\n",
        "        else:\n",
        "                next_position = (state[0], state[1] + 1)\n",
        "        # Keep the next state within specified range. \n",
        "        if (next_position[0] >= 0) and (next_position[0] <= (Grid_Rows -1)):\n",
        "            if (next_position[1] >= 0) and (next_position[1] <= (Grid_Col -1)):\n",
        "                #if next_position != (1, 1):\n",
        "                    reward_ = self.get_reward(next_position)\n",
        "                    return next_position, reward_\n",
        "                    \n",
        "        return state, 0\n",
        "    \n",
        "    def get_reward(self, state):\n",
        "        self.done = False\n",
        "        if state == WIN_STATE:\n",
        "            self.reward = 100\n",
        "            self.done = True\n",
        "        \n",
        "        elif state == LOSE_STATE:\n",
        "            self.reward = -100\n",
        "            self.done = True\n",
        "        elif (state[0] == 0) and (state[1] <= (Grid_Col -1)):\n",
        "            self.reward = -10\n",
        "        elif (state[0] <= (Grid_Col -1)) and (state[1] == 0 ):\n",
        "            self.reward = -10\n",
        "        else:\n",
        "            self.reward = 0\n",
        "            self.done = False\n",
        "        return self.reward\n",
        "    \n",
        "    def step(self, state, action):\n",
        "        next_state, reward_ = self.move(state, action)\n",
        "        self.state = next_state\n",
        "        \n",
        "        return next_state, reward_, self.done\n",
        "        \n",
        "    \n",
        "    def render(self, next_state):\n",
        "        print(\"---------------------------------\")\n",
        "        print(\"Current State is \", np.asarray(next_state))\n",
        "        print(\"The reward is   \",self.reward)\n",
        "        print(\"---------------------------------\")\n",
        "        \n",
        "    def reset(self):\n",
        "        self.state = (np.random.randint(0, Grid_Rows), np.random.randint(0, Grid_Col))\n",
        "        #self.state = (2,2)\n",
        "        return self.state\n",
        "        \n",
        "        \n",
        "    def render_grid(self):\n",
        "        print(f\"Grid Size = {Grid_Rows} X {Grid_Col} :: Total States = {Grid_Col*Grid_Rows}\")\n",
        "        print(f\"Agent = A : Win State = W : Lose State = L\")\n",
        "        self.print_grid[self.state] = 1\n",
        "        for i in range(0, Grid_Rows):\n",
        "            print(\"-----------------------------------------------------------------\")\n",
        "            out = '| '\n",
        "            for j in range(0, Grid_Col):\n",
        "                if self.print_grid[i, j] == 1:\n",
        "                    token = 'A'\n",
        "                if self.print_grid[i, j] == 2:\n",
        "                    token = 'L'\n",
        "                if self.print_grid[i, j] == 0:\n",
        "                    token = ' '\n",
        "                if self.print_grid[i, j] == 9:\n",
        "                    token = 'W'\n",
        "                out += token + ' | '\n",
        "            print(out)\n",
        "        print(\"-----------------------------------------------------------------\")\n",
        "        \n",
        "        self.print_grid[self.state] = 0\n",
        "        self.print_grid[WIN_STATE] = 9\n",
        "        self.print_grid[LOSE_STATE] = 2\n",
        "        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gWiP5xKpdu7S"
      },
      "source": [
        "\n",
        "# Test Section\n",
        "\n",
        "env = grid_word()\n",
        "print(env.action_space.n)\n",
        "state = env.reset()\n",
        "for i in range(100):\n",
        "    action = env.action_space.sample()\n",
        "    n_state, reward, done = env.step(state, action)\n",
        "    print(f\"State = {state}; Action = {action}; Next State = {n_state}; Reward = {reward}; Done = {done} \")\n",
        "    state = n_state\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eM43-onkfwoX"
      },
      "source": [
        "## **Value Iteration **"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9_TVvM24e4Qf"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "Q_table = {}\n",
        "for i in range(Grid_Rows):\n",
        "    for j in range(Grid_Col):\n",
        "        Q_table[(i, j)] = [0,0,0,0,0]\n",
        "\n",
        "\n",
        "all_states = []\n",
        "for i in range(Grid_Rows):\n",
        "    for j in range(Grid_Col):\n",
        "        all_states.append((i,j))\n",
        "\n",
        "env = grid_word()\n",
        "\n",
        "def get_value_table(env):\n",
        "\n",
        "    #set the number of iterations\n",
        "    num_iterations = 2000\n",
        "    \n",
        "    #set the threshold number for checking the convergence of the value function\n",
        "    threshold = 1e-40\n",
        "    \n",
        "    #we also set the discount factor\n",
        "    gamma = 0.1\n",
        "    con_value = 0\n",
        "    c_act = 0\n",
        "    \n",
        "    #now, we will initialize the value table, with the value of all states to zero\n",
        "    value_table = np.zeros((Grid_Rows, Grid_Col))\n",
        "    \n",
        "    \n",
        "    #for every iteration\n",
        "    for i in range(num_iterations):\n",
        "        \n",
        "        #table (state values) from the previous iteration\n",
        "        updated_value_table = np.copy(value_table)\n",
        "        \n",
        "        \n",
        "        for s in all_states:\n",
        "            for a in env.all_actions :\n",
        "                    s_, reward, done = env.step(s, a)\n",
        "                    Q_table[s][a] = reward + gamma * updated_value_table[s_]\n",
        "            value_table[s] = max(Q_table[s])\n",
        "            \n",
        "            #if done == True:\n",
        "            #    env.reset()\n",
        "        \n",
        "        if (np.sum(np.fabs(updated_value_table - value_table)) <= threshold):\n",
        "            \n",
        "            c_act = a\n",
        "            con_value = i \n",
        "            break\n",
        "    \n",
        "    return value_table, con_value, c_act\n",
        "\n",
        "def get_policy(value_table):\n",
        "    \n",
        "    #set the discount factor\n",
        "    gamma = 1.0\n",
        "    policy = np.zeros((Grid_Rows, Grid_Col))\n",
        "    for s in all_states:\n",
        "            for a in env.all_actions :\n",
        "                s_, reward, done = env.step(s, a)                \n",
        "                Q_table[s][a] = reward + gamma * value_table[s_] \n",
        "            policy[s] = np.argmax(Q_table[s]) \n",
        "            if done == True:\n",
        "                    env.reset()\n",
        "    \n",
        "    \n",
        "    \n",
        "    return policy\n",
        "\n",
        "\n",
        "convergance_value = []\n",
        "calculate_conv = []\n",
        "con_answer = []\n",
        "for rep in range(20):\n",
        "    value_table, con_value, c_act = get_value_table(env) # Based on value iteration\n",
        "    \n",
        "    optimal_policy = get_policy(value_table)\n",
        "    calculate_conv.append(con_value)\n",
        "    \n",
        "    if rep % 1 == 0:\n",
        "        val = (sum(calculate_conv)/1)\n",
        "        convergance_value.append(val)\n",
        "        calculate_conv = []\n",
        "        con_answer.append(c_act)\n",
        "        #print(val)\n",
        "        \n",
        "        \n",
        "\n",
        "print(\"Computed Value Table based on Value Iteration\")\n",
        "print(value_table)\n",
        " # Based on value table\n",
        "print(\"Optimal Policy Table based on Value Iteration\")\n",
        "print(optimal_policy)\n",
        "\n",
        "\n",
        "print(\"Average value for convergance \", round((sum(convergance_value)/len(convergance_value))))\n",
        "\n",
        "plt.figure(figsize=(10,5)) \n",
        "plt.plot(convergance_value, \"g--\", markersize=20, linewidth=5)\n",
        "plt.legend([\"Convergance Value\"], fontsize=20)\n",
        "plt.title(\"Iteration taken to converge [Value Iteration]\", fontsize=20)\n",
        "plt.xlabel(\"Repeatations\", fontsize=20)\n",
        "plt.ylabel(\"Convergance Point\", fontsize=20)\n",
        "plt.xlim(0,20)\n",
        "plt.grid()\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(10,5)) \n",
        "plt.plot(con_answer, \"g--\", markersize=20, linewidth=5)\n",
        "plt.legend([\"Convergance Answer\"], fontsize=20)\n",
        "plt.title(\"Action Value at convergance [Value Iteration]\", fontsize=20)\n",
        "plt.xlabel(\"Repeatations\", fontsize=20)\n",
        "plt.ylabel(\"Action Value\", fontsize=20)\n",
        "plt.xlim(0,20)\n",
        "plt.grid()\n",
        "plt.show()\n",
        "\n",
        "#np.save(\"Convergance_Action_Value_Itetration_0_7g.npy\",con_answer , allow_pickle=True)\n",
        "#np.save(\"Convergance_Value_Value_Itetration_10_10g.npy\", convergance_value, allow_pickle=True)\n",
        "\n",
        "\n",
        "\n",
        "#--------------------------------------------------------------------------------  \n",
        "# Value Iteration Play\n",
        "\n",
        "\n",
        "state = env.reset()\n",
        "episode_count = []\n",
        "win_count = []\n",
        "step_count = []\n",
        "lose_count = []\n",
        "count_w = 0\n",
        "count_l = 0\n",
        "i = 0\n",
        "for j in range(100):\n",
        "    state = env.reset()\n",
        "    episode_count.append(j)\n",
        "    win_count.append(count_w)\n",
        "    lose_count.append(count_l)\n",
        "    step_count.append(i)\n",
        "    for i in range(100):\n",
        "        action = optimal_policy[state]\n",
        "        #print(state, action)\n",
        "        n_state, reward, done = env.step(state, action)\n",
        "        \n",
        "        state = n_state\n",
        "        if env.done:\n",
        "            if reward == 100:\n",
        "                count_w += 1\n",
        "            if reward == -100:\n",
        "                count_l += 1\n",
        "            \n",
        "            break\n",
        "    \n",
        "\n",
        "print(\"Game Results Optimal Policy\")\n",
        "print(f\"Total Games played 100: [Win Games {count_w}]: [Lose Games {count_l}]\")\n",
        "avg_step = sum(step_count)/100\n",
        "print(f\"Average Steps to win 100 games = {round(avg_step)}\")\n",
        "\n",
        "print(f\"Optimal Policy Accuracy  [{(count_w/100)*100}%]\")\n",
        "#Visualization\n",
        "plt.figure(figsize=(15,10)) \n",
        "plt.bar(episode_count, step_count)\n",
        "plt.title(\"Steps Taken to Solve the Grid Word by [Value Iteration]\", fontsize=20)\n",
        "plt.xlabel(\"Game Number\", fontsize=20)\n",
        "plt.ylabel(\"Win Step Number\", fontsize=20)\n",
        "plt.xlim(0,100)\n",
        "plt.grid()\n",
        "plt.show()\n",
        "       \n",
        "plt.figure(figsize=(15,10))   \n",
        "plt.plot(step_count, \"g-\", markersize=20, linewidth=5)\n",
        "plt.title(\"Steps Taken to Solve the Grid Word by [Value Iteration]\", fontsize=20)\n",
        "plt.xlabel(\"Game Number\", fontsize=20)\n",
        "plt.ylabel(\"Win Step Number\", fontsize=20)\n",
        "plt.xlim(0,100)\n",
        "plt.grid()\n",
        "plt.show()\n",
        " \n",
        "\n",
        "plt.figure(figsize=(15,10)) \n",
        "plt.plot(win_count, \"g-\", markersize=20, linewidth=5)\n",
        "plt.plot(lose_count, \"r-\", markersize=20, linewidth=5)\n",
        "plt.title(\"Games Win by [Value Iteration] : Total Games = 100\", fontsize=20)\n",
        "plt.legend([\"WIN\", \"LOSE\"], fontsize=20)\n",
        "plt.ylabel(\"Win Games\", fontsize=20)\n",
        "plt.xlabel(\"Game Number\", fontsize=20)\n",
        "plt.xlim(0,100)\n",
        "plt.grid()\n",
        "plt.show()\n",
        "\n",
        "#--------------------------------------------------------------------------------\n",
        "# Random Policy Play\n",
        "\n",
        "\n",
        "state = env.reset()\n",
        "episode_count = []\n",
        "win_count = []\n",
        "step_count = []\n",
        "lose_count = []\n",
        "count_w = 0\n",
        "count_l = 0\n",
        "i = 0\n",
        "for j in range(100):\n",
        "    state = env.reset()\n",
        "    episode_count.append(j)\n",
        "    win_count.append(count_w)\n",
        "    lose_count.append(count_l)\n",
        "    step_count.append(i)\n",
        "    for i in range(100):\n",
        "        action = env.action_space.sample() #optimal_policy[state]\n",
        "        #print(state, action)\n",
        "        n_state, reward, done = env.step(state, action)\n",
        "        \n",
        "        state = n_state\n",
        "        if env.done:\n",
        "            if reward == 100:\n",
        "                count_w += 1\n",
        "            if reward == -100:\n",
        "                count_l += 1\n",
        "            \n",
        "            break\n",
        "    \n",
        "\n",
        "env.render_grid()\n",
        "\n",
        "\n",
        "print(\"Game Results Random Policy\")\n",
        "print(f\"Total Games played 100: [Win Games {count_w}]: [Lose Games {count_l}]\")\n",
        "avg_step = sum(step_count)/100\n",
        "print(f\"Average Steps to win 100 games = {round(avg_step)}\")\n",
        "\n",
        "print(f\"Random Policy Accuracy  [{(count_w/100)*100}%]\")\n",
        "\n",
        "#Visualization\n",
        "plt.figure(figsize=(15,10)) \n",
        "plt.bar(episode_count, step_count)\n",
        "plt.title(\"Steps Taken to Solve the Grid Word by [Random Policy]\", fontsize=20)\n",
        "plt.xlabel(\"Game Number\", fontsize=20)\n",
        "plt.ylabel(\"Win Step Number\", fontsize=20)\n",
        "plt.xlim(0,100)\n",
        "plt.grid()\n",
        "plt.show()\n",
        "       \n",
        "plt.figure(figsize=(15,10))   \n",
        "plt.plot(step_count, \"g-\", markersize=20, linewidth=5)\n",
        "plt.title(\"Steps Taken to Solve the Grid Word by [Random Policy]\", fontsize=20)\n",
        "plt.xlabel(\"Game Number\", fontsize=20)\n",
        "plt.ylabel(\"Win Step Number\", fontsize=20)\n",
        "plt.xlim(0,100)\n",
        "plt.grid()\n",
        "plt.show()\n",
        " \n",
        "\n",
        "plt.figure(figsize=(15,10)) \n",
        "plt.plot(win_count, \"g-\", markersize=20, linewidth=5)\n",
        "plt.plot(lose_count, \"r-\", markersize=20, linewidth=5)\n",
        "plt.title(\"Games Win by [Random Policy] : Total Games = 100\", fontsize=20)\n",
        "plt.legend([\"WIN\", \"LOSE\"], fontsize=20)\n",
        "plt.ylabel(\"Win Games\", fontsize=20)\n",
        "plt.xlabel(\"Game Number\", fontsize=20)\n",
        "plt.xlim(0,100)\n",
        "plt.grid()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ezr4471gf1ee"
      },
      "source": [
        "## **Policy Iteration**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rp4ia2qLgG7l"
      },
      "source": [
        "Q_table = {}\n",
        "for i in range(Grid_Rows):\n",
        "    for j in range(Grid_Col):\n",
        "        Q_table[(i, j)] = [0,0,0,0]\n",
        "\n",
        "#print(Q_table)        \n",
        "state_size = Grid_Rows * Grid_Col\n",
        "#action_size = 4\n",
        "all_state = []\n",
        "for i in range(Grid_Rows):\n",
        "    for j in range(Grid_Col):\n",
        "        all_state.append((i,j))\n",
        "#policy_ = np.zeros((Grid_Rows, Grid_Col))\n",
        "        \n",
        "#value_table = np.zeros((Grid_Rows, Grid_Col))\n",
        "#updated_value_table = np.copy(value_table)\n",
        "env = grid_word()\n",
        "state = env.reset()\n",
        "GAMMA = 0.1\n",
        "threshold = 1e-20\n",
        "\n",
        "#num_iterations = 1000\n",
        "#\n",
        "\n",
        "#num_steps = 100\n",
        "\n",
        "\n",
        "def get_value_function(policy):\n",
        "    num_iterations = 1000\n",
        "    threshold = 1e-40\n",
        "    gamma = GAMMA\n",
        "    value_table = np.zeros((Grid_Rows, Grid_Col))\n",
        "# First we computer the value table based on the random policy\n",
        "    for i in range(num_iterations):\n",
        "        updated_value_table = np.copy(value_table)\n",
        "        for s in all_state:\n",
        "                action = policy[s] #env.action_space.sample() #\n",
        "                s_, reward = env.move(s, action)\n",
        "                value_table[s] = reward + gamma * updated_value_table[s_]\n",
        "        \n",
        "            \n",
        "        if (np.sum(np.fabs(updated_value_table - value_table)) <= threshold):\n",
        "            break\n",
        "    return value_table\n",
        "\n",
        "def extract_policy(value_table):\n",
        "    gamma = GAMMA\n",
        "    policy_ = np.zeros((Grid_Rows, Grid_Col))\n",
        "    for s in all_state:\n",
        "        for a in env.all_actions:\n",
        "            s_, reward = env.move(s, a)\n",
        "            Q_table[s][a] = reward + gamma * value_table[s_]\n",
        "        policy_[s] = np.argmax(Q_table[s])\n",
        "            \n",
        "    return policy_, a\n",
        "\n",
        "def policy_iteration(env):\n",
        "    con_value = 0\n",
        "    c_act = 0\n",
        "    num_iterations = 1000\n",
        "    policy_  = np.zeros((Grid_Rows, Grid_Col))\n",
        "    for i in range(num_iterations):\n",
        "        value_function = get_value_function(policy_)\n",
        "        new_policy, a = extract_policy(value_function)\n",
        "        if (np.all(policy_ == new_policy)):\n",
        "            c_act = a\n",
        "            con_value = i\n",
        "            break\n",
        "        policy_ = new_policy\n",
        "    return policy_, con_value, a\n",
        "\n",
        "env = grid_word()\n",
        "convergance_value = []\n",
        "calculate_conv = []\n",
        "con_answer = []\n",
        "for rep in range(20):\n",
        "    optimal_policy, con_value, a = policy_iteration(env)\n",
        "    calculate_conv.append(con_value)\n",
        "    \n",
        "    if rep % 1 == 0:\n",
        "        val = (sum(calculate_conv)/1)\n",
        "        convergance_value.append(val)\n",
        "        calculate_conv = []\n",
        "        con_answer.append(a)\n",
        "\n",
        "\n",
        "\n",
        "#print(\"Computed Value Table based on Policy Iteration\")\n",
        "#print(value_table)\n",
        " # Based on value table\n",
        "print(\"Optimal Policy Table based on Policy Iteration\")\n",
        "print(optimal_policy)\n",
        "\n",
        "\n",
        "print(\"Average value for convergence \", round((sum(convergance_value)/len(convergance_value))))\n",
        "\n",
        "plt.figure(figsize=(10,5)) \n",
        "plt.plot(convergance_value, \"g--\", markersize=20, linewidth=5)\n",
        "plt.legend([\"Convergence Value\"], fontsize=20)\n",
        "plt.title(\"Iteration taken to converge [Policy Iteration]\", fontsize=20)\n",
        "plt.xlabel(\"Repeatations\", fontsize=20)\n",
        "plt.ylabel(\"Convergence Point\", fontsize=20)\n",
        "plt.xlim(0,20)\n",
        "plt.grid()\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(10,5)) \n",
        "plt.plot(con_answer, \"g--\", markersize=20, linewidth=5)\n",
        "plt.legend([\"Convergence Answer\"], fontsize=20)\n",
        "plt.title(\"Action Value at convergence [Policy Iteration]\", fontsize=20)\n",
        "plt.xlabel(\"Repeatations\", fontsize=20)\n",
        "plt.ylabel(\"Action Value\", fontsize=20)\n",
        "plt.xlim(0,20)\n",
        "plt.grid()\n",
        "plt.show()\n",
        "\n",
        "#np.save(\"Convergance_Action_Policy_Itetration_0_7g.npy\",con_answer , allow_pickle=True)\n",
        "#np.save(\"Convergance_Value_Policy_Itetration_10_10g.npy\", convergance_value, allow_pickle=True)\n",
        "\n",
        "\n",
        "  \n",
        "# Value Iteration Play\n",
        "\n",
        "\n",
        "state = env.reset()\n",
        "episode_count = []\n",
        "win_count = []\n",
        "step_count = []\n",
        "lose_count = []\n",
        "count_w = 0\n",
        "count_l = 0\n",
        "i = 0\n",
        "for j in range(100):\n",
        "    state = env.reset()\n",
        "    episode_count.append(j)\n",
        "    win_count.append(count_w)\n",
        "    lose_count.append(count_l)\n",
        "    step_count.append(i)\n",
        "    for i in range(100):\n",
        "        action = optimal_policy[state]\n",
        "        #print(state, action)\n",
        "        n_state, reward, done = env.step(state, action)\n",
        "        \n",
        "        state = n_state\n",
        "        if env.done:\n",
        "            if reward == 100:\n",
        "                count_w += 1\n",
        "            if reward == -100:\n",
        "                count_l += 1\n",
        "            \n",
        "            break\n",
        "    \n",
        "\n",
        "print(\"Game Results Optimal Policy\")\n",
        "print(f\"Total Games played 100: [Win Games {count_w}]: [Lose Games {count_l}]\")\n",
        "avg_step = sum(step_count)/100\n",
        "print(f\"Average Steps to win 100 games = {round(avg_step)}\")\n",
        "\n",
        "print(f\"Optimal Policy Accuracy  [{(count_w/100)*100}%]\")\n",
        "#Visualization\n",
        "plt.figure(figsize=(15,10)) \n",
        "plt.bar(episode_count, step_count)\n",
        "plt.title(\"Steps Taken to Solve the Grid Word by [Policy Iteration]\", fontsize=20)\n",
        "plt.xlabel(\"Game Number\", fontsize=20)\n",
        "plt.ylabel(\"Win Step Number\", fontsize=20)\n",
        "plt.xlim(0,100)\n",
        "plt.grid()\n",
        "plt.show()\n",
        "       \n",
        "plt.figure(figsize=(15,10))   \n",
        "plt.plot(step_count, \"g-\", markersize=20, linewidth=5)\n",
        "plt.title(\"Steps Taken to Solve the Grid Word by [Policy Iteration]\", fontsize=20)\n",
        "plt.xlabel(\"Game Number\", fontsize=20)\n",
        "plt.ylabel(\"Win Step Number\", fontsize=20)\n",
        "plt.xlim(0,100)\n",
        "plt.grid()\n",
        "plt.show()\n",
        " \n",
        "\n",
        "plt.figure(figsize=(15,10)) \n",
        "plt.plot(win_count, \"g-\", markersize=20, linewidth=5)\n",
        "plt.plot(lose_count, \"r-\", markersize=20, linewidth=5)\n",
        "plt.title(\"Games Win by [Policy Iteration] : Total Games = 100\", fontsize=20)\n",
        "plt.legend([\"WIN\", \"LOSE\"], fontsize=20)\n",
        "plt.ylabel(\"Win Games\", fontsize=20)\n",
        "plt.xlabel(\"Game Number\", fontsize=20)\n",
        "plt.xlim(0,100)\n",
        "plt.grid()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "state = env.reset()\n",
        "episode_count = []\n",
        "win_count = []\n",
        "step_count = []\n",
        "lose_count = []\n",
        "count_w = 0\n",
        "count_l = 0\n",
        "i = 0\n",
        "for j in range(100):\n",
        "    state = env.reset()\n",
        "    episode_count.append(j)\n",
        "    win_count.append(count_w)\n",
        "    lose_count.append(count_l)\n",
        "    step_count.append(i)\n",
        "    for i in range(100):\n",
        "        action = env.action_space.sample() #optimal_policy[state]\n",
        "        #print(state, action)\n",
        "        n_state, reward, done = env.step(state, action)\n",
        "        \n",
        "        state = n_state\n",
        "        if env.done:\n",
        "            if reward == 100:\n",
        "                count_w += 1\n",
        "            if reward == -100:\n",
        "                count_l += 1\n",
        "            \n",
        "            break\n",
        "    \n",
        "\n",
        "env.render_grid()\n",
        "\n",
        "\n",
        "print(\"Game Results Random Policy\")\n",
        "print(f\"Total Games played 100: [Win Games {count_w}]: [Lose Games {count_l}]\")\n",
        "avg_step = sum(step_count)/100\n",
        "print(f\"Average Steps to win 100 games = {round(avg_step)}\")\n",
        "\n",
        "print(f\"Random Policy Accuracy  [{(count_w/100)*100}%]\")\n",
        "\n",
        "#Visualization\n",
        "plt.figure(figsize=(15,10)) \n",
        "plt.bar(episode_count, step_count)\n",
        "plt.title(\"Steps Taken to Solve the Grid Word by [Random Policy]\", fontsize=20)\n",
        "plt.xlabel(\"Game Number\", fontsize=20)\n",
        "plt.ylabel(\"Win Step Number\", fontsize=20)\n",
        "plt.xlim(0,100)\n",
        "plt.grid()\n",
        "plt.show()\n",
        "       \n",
        "plt.figure(figsize=(15,10))   \n",
        "plt.plot(step_count, \"g-\", markersize=20, linewidth=5)\n",
        "plt.title(\"Steps Taken to Solve the Grid Word by [Random Policy]\", fontsize=20)\n",
        "plt.xlabel(\"Game Number\", fontsize=20)\n",
        "plt.ylabel(\"Win Step Number\", fontsize=20)\n",
        "plt.xlim(0,100)\n",
        "plt.grid()\n",
        "plt.show()\n",
        " \n",
        "\n",
        "plt.figure(figsize=(15,10)) \n",
        "plt.plot(win_count, \"g-\", markersize=20, linewidth=5)\n",
        "plt.plot(lose_count, \"r-\", markersize=20, linewidth=5)\n",
        "plt.title(\"Games Win by [Random Policy] : Total Games = 100\", fontsize=20)\n",
        "plt.legend([\"WIN\", \"LOSE\"], fontsize=20)\n",
        "plt.ylabel(\"Win Games\", fontsize=20)\n",
        "plt.xlabel(\"Game Number\", fontsize=20)\n",
        "plt.xlim(0,100)\n",
        "plt.grid()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}